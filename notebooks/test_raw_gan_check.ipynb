{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:20.940433Z",
     "start_time": "2019-08-20T23:54:20.935154Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:21.322854Z",
     "start_time": "2019-08-20T23:54:21.319596Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse, json\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:25.959033Z",
     "start_time": "2019-08-20T23:54:21.591416Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from data.pointcloud_dataset import load_one_class_under_folder\n",
    "from utils.dirs import mkdir_and_rename\n",
    "from utils.tf import reset_tf_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:26.546477Z",
     "start_time": "2019-08-20T23:54:26.533273Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'data': {\n",
    "        'data_root':\n",
    "        '/orion/u/jiangthu/projects/latent_3d_points/data/shape_net_core_uniform_samples_2048',\n",
    "        'class_name': 'airplane',\n",
    "        'n_thread': 20\n",
    "    },\n",
    "    'model': {\n",
    "        'type': 'wgan',\n",
    "        'num_points': 2048,\n",
    "        'noise_dim': 128,\n",
    "        'noise_params': {\n",
    "            'mu': 0,\n",
    "            'sigma': 0.2\n",
    "        }\n",
    "    },\n",
    "    'train': {\n",
    "        'batch_size': 50,\n",
    "        'learning_rate': 0.0001,\n",
    "        'beta': 0.5,\n",
    "        'z_rotate': False,\n",
    "        'saver_step': 100\n",
    "    },\n",
    "    'path': {\n",
    "        'train_root': './experiments',\n",
    "        'experiment_name': 'single_class_gan_chair_noise128'\n",
    "    }\n",
    "}\n",
    "train_dir = osp.join(opt['path']['train_root'], opt['path']['experiment_name'])\n",
    "train_opt = opt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:37.209809Z",
     "start_time": "2019-08-20T23:54:28.943476Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.tf import leaky_relu\n",
    "from utils.tf import expand_scope_by_name\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.layers.core import fully_connected, dropout\n",
    "from tflearn.layers.conv import conv_1d\n",
    "from utils.tf import expand_scope_by_name, replicate_parameter_for_all_layers\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:37.241383Z",
     "start_time": "2019-08-20T23:54:37.211973Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoder_with_convs_and_symmetry(in_signal,\n",
    "                                    init_list,\n",
    "                                    n_filters=[64, 128, 256, 1024],\n",
    "                                    filter_sizes=[1],\n",
    "                                    strides=[1],\n",
    "                                    non_linearity=tf.nn.relu,\n",
    "                                    weight_decay=0.001,\n",
    "                                    symmetry=tf.reduce_max,\n",
    "                                    regularizer=None,\n",
    "                                    scope=None,\n",
    "                                    reuse=False,\n",
    "                                    padding='same',\n",
    "                                    verbose=False,\n",
    "                                    conv_op=conv_1d):\n",
    "    '''An Encoder (recognition network), which maps inputs onto a latent space.\n",
    "    '''\n",
    "\n",
    "    if verbose:\n",
    "        print('Building Encoder')\n",
    "\n",
    "    n_layers = len(n_filters)\n",
    "    filter_sizes = replicate_parameter_for_all_layers(filter_sizes, n_layers)\n",
    "    strides = replicate_parameter_for_all_layers(strides, n_layers)\n",
    "\n",
    "    if n_layers < 2:\n",
    "        raise ValueError('More than 1 layers are expected.')\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            layer = in_signal\n",
    "\n",
    "        name = 'encoder_conv_layer_' + str(i)\n",
    "        scope_i = expand_scope_by_name(scope, name)\n",
    "        layer = conv_op(layer,\n",
    "                        nb_filter=n_filters[i],\n",
    "                        filter_size=filter_sizes[i],\n",
    "                        strides=strides[i],\n",
    "                        regularizer=regularizer,\n",
    "                        weight_decay=weight_decay,\n",
    "                        name=name,\n",
    "                        reuse=reuse,\n",
    "                        scope=scope_i,\n",
    "                        padding=padding,\n",
    "                        weights_init=tf.constant_initializer(init_list[i][0]),\n",
    "                        bias_init=tf.constant_initializer(init_list[i][1]))\n",
    "\n",
    "        if non_linearity is not None:\n",
    "            layer = non_linearity(layer)\n",
    "\n",
    "        if verbose:\n",
    "            print(layer)\n",
    "            print('output size:', np.prod(layer.get_shape().as_list()[1:]),\n",
    "                  '\\n')\n",
    "\n",
    "    if symmetry is not None:\n",
    "        layer = symmetry(layer, axis=1)\n",
    "        if verbose:\n",
    "            print(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def decoder_with_fc_only(latent_signal,\n",
    "                         init_list,\n",
    "                         layer_sizes=[],\n",
    "                         non_linearity=tf.nn.relu,\n",
    "                         regularizer=None,\n",
    "                         weight_decay=0.001,\n",
    "                         reuse=False,\n",
    "                         scope=None,\n",
    "                         verbose=False):\n",
    "    '''A decoding network which maps points from the latent space back onto the data space.\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('Building Decoder')\n",
    "\n",
    "    n_layers = len(layer_sizes)\n",
    "\n",
    "    if n_layers < 2:\n",
    "        raise ValueError(\n",
    "            'For an FC decoder with single a layer use simpler code.')\n",
    "\n",
    "    for i in range(0, n_layers - 1):\n",
    "        name = 'decoder_fc_' + str(i)\n",
    "        scope_i = expand_scope_by_name(scope, name)\n",
    "\n",
    "        if i == 0:\n",
    "            layer = latent_signal\n",
    "\n",
    "        layer = fully_connected(\n",
    "            layer,\n",
    "            layer_sizes[i],\n",
    "            activation='linear',\n",
    "            weights_init=tf.constant_initializer(init_list[i][0]),\n",
    "            bias_init=tf.constant_initializer(init_list[i][1]),\n",
    "            name=name,\n",
    "            regularizer=regularizer,\n",
    "            weight_decay=weight_decay,\n",
    "            reuse=reuse,\n",
    "            scope=scope_i)\n",
    "\n",
    "        if verbose:\n",
    "            print(name,\n",
    "                  'FC params = ',\n",
    "                  np.prod(layer.W.get_shape().as_list()) +\n",
    "                  np.prod(layer.b.get_shape().as_list()),\n",
    "                  end=' ')\n",
    "\n",
    "        if non_linearity is not None:\n",
    "            layer = non_linearity(layer)\n",
    "\n",
    "        if verbose:\n",
    "            print(layer)\n",
    "            print('output size:', np.prod(layer.get_shape().as_list()[1:]),\n",
    "                  '\\n')\n",
    "\n",
    "    # Last decoding layer never has a non-linearity.\n",
    "    name = 'decoder_fc_' + str(n_layers - 1)\n",
    "    scope_i = expand_scope_by_name(scope, name)\n",
    "    layer = fully_connected(layer,\n",
    "                            layer_sizes[n_layers - 1],\n",
    "                            activation='linear',\n",
    "                            weights_init=tf.constant_initializer(init_list[-1][0]),\n",
    "                            bias_init=tf.constant_initializer(init_list[-1][1]),\n",
    "                            name=name,\n",
    "                            regularizer=regularizer,\n",
    "                            weight_decay=weight_decay,\n",
    "                            reuse=reuse,\n",
    "                            scope=scope_i)\n",
    "    if verbose:\n",
    "        print(name,\n",
    "              'FC params = ',\n",
    "              np.prod(layer.W.get_shape().as_list()) +\n",
    "              np.prod(layer.b.get_shape().as_list()),\n",
    "              end=' ')\n",
    "\n",
    "    if verbose:\n",
    "        print(layer)\n",
    "        print('output size:', np.prod(layer.get_shape().as_list()[1:]), '\\n')\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def mlp_discriminator(in_signal,\n",
    "                      cov_init_list,\n",
    "                      fc_init_list,\n",
    "                      non_linearity=tf.nn.relu,\n",
    "                      reuse=False,\n",
    "                      scope=None):\n",
    "    ''' used in nips submission.\n",
    "    '''\n",
    "    encoder_args = {\n",
    "        'n_filters': [64, 128, 256, 256, 512],\n",
    "        'filter_sizes': [1, 1, 1, 1, 1],\n",
    "        'strides': [1, 1, 1, 1, 1]\n",
    "    }\n",
    "    encoder_args['reuse'] = reuse\n",
    "    encoder_args['scope'] = scope\n",
    "    encoder_args['non_linearity'] = non_linearity\n",
    "    layer = encoder_with_convs_and_symmetry(in_signal, cov_init_list, weight_decay=0.0,\n",
    "                                            **encoder_args)\n",
    "\n",
    "    name = 'decoding_logits'\n",
    "    scope_e = expand_scope_by_name(scope, name)\n",
    "    d_logit = decoder_with_fc_only(layer,\n",
    "                                   fc_init_list,\n",
    "                                   layer_sizes=[128, 64, 1],\n",
    "                                   reuse=reuse,\n",
    "                                   scope=scope_e,\n",
    "                                   weight_decay=0.0)\n",
    "    d_prob = tf.nn.sigmoid(d_logit)\n",
    "    return d_prob, d_logit\n",
    "\n",
    "\n",
    "def point_cloud_generator(z,\n",
    "                          pc_dims,\n",
    "                          init_list,\n",
    "                          layer_sizes=[64, 128, 512, 1024],\n",
    "                          non_linearity=tf.nn.relu):\n",
    "    ''' used in nips submission.\n",
    "    '''\n",
    "\n",
    "    n_points, dummy = pc_dims\n",
    "    if (dummy != 3):\n",
    "        raise ValueError()\n",
    "\n",
    "    out_signal = decoder_with_fc_only(z,\n",
    "                                      init_list[:-1],\n",
    "                                      layer_sizes=layer_sizes,\n",
    "                                      non_linearity=non_linearity, weight_decay=0.0)\n",
    "    out_signal = non_linearity(out_signal)\n",
    "\n",
    "\n",
    "    out_signal = fully_connected(out_signal,\n",
    "                                 np.prod([n_points, 3]),\n",
    "                                 activation='linear',\n",
    "                                 weights_init=tf.constant_initializer(init_list[-1][0]),\n",
    "                                 bias_init=tf.constant_initializer(init_list[-1][1]),\n",
    "                                 weight_decay=0.0)\n",
    "    out_signal = tf.reshape(out_signal, [-1, n_points, 3])\n",
    "    return out_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:37.261100Z",
     "start_time": "2019-08-20T23:54:37.243861Z"
    }
   },
   "outputs": [],
   "source": [
    "from trainers.gan import GAN\n",
    "from tflearn import is_training\n",
    "class PGAN(GAN):\n",
    "    '''Gradient Penalty.\n",
    "    https://arxiv.org/abs/1704.00028\n",
    "    '''\n",
    "\n",
    "    def __init__(self, name, learning_rate, lam, n_output, noise_dim, discriminator, generator, beta=0.5, gen_kwargs={}, disc_kwargs={}, graph=None):\n",
    "\n",
    "        GAN.__init__(self, name, graph)\n",
    "        \n",
    "        self.noise_dim = noise_dim\n",
    "        self.n_output = n_output\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "    \n",
    "        with tf.variable_scope(name):\n",
    "            self.noise = tf.placeholder(tf.float32, shape=[None, noise_dim])            # Noise vector.\n",
    "            self.real_pc = tf.placeholder(tf.float32, shape=[None] + self.n_output)     # Ground-truth.\n",
    "\n",
    "            with tf.variable_scope('generator'):\n",
    "                self.generator_out = self.generator(self.noise, self.n_output, **gen_kwargs)\n",
    "                \n",
    "            with tf.variable_scope('discriminator') as scope:\n",
    "                self.real_prob, self.real_logit = self.discriminator(self.real_pc, scope=scope, **disc_kwargs)\n",
    "                self.synthetic_prob, self.synthetic_logit = self.discriminator(self.generator_out, reuse=True, scope=scope, **disc_kwargs)\n",
    "            \n",
    "            \n",
    "            # Compute WGAN losses\n",
    "            self.loss_d_logit = tf.reduce_mean(self.synthetic_logit) - tf.reduce_mean(self.real_logit)\n",
    "            self.loss_g = -tf.reduce_mean(self.synthetic_logit)\n",
    "\n",
    "#             # Compute gradient penalty at interpolated points\n",
    "#             ndims = self.real_pc.get_shape().ndims\n",
    "#             batch_size = tf.shape(self.real_pc)[0]\n",
    "#             alpha = 0.5\n",
    "#             differences = self.generator_out - self.real_pc\n",
    "#             interpolates = self.real_pc + (alpha * differences)\n",
    "\n",
    "#             with tf.variable_scope('discriminator') as scope:\n",
    "#                 gradients = tf.gradients(self.discriminator(interpolates, reuse=True, scope=scope, **disc_kwargs)[1], [interpolates])[0]\n",
    "\n",
    "#             # Reduce over all but the first dimension\n",
    "#             slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=list(range(1, ndims))))\n",
    "#             self.gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
    "#             self.loss_d = self.loss_d_logit + lam * self.gradient_penalty\n",
    "            self.loss_d = self.loss_d_logit\n",
    "\n",
    "            train_vars = tf.trainable_variables()\n",
    "            d_params = [v for v in train_vars if v.name.startswith(name + '/discriminator/')]\n",
    "            g_params = [v for v in train_vars if v.name.startswith(name + '/generator/')]\n",
    "            \n",
    "            self.opt_d = self.optimizer(learning_rate, beta, self.loss_d, d_params)\n",
    "            self.opt_g = self.optimizer(learning_rate, beta, self.loss_g, g_params)\n",
    "#             self.optimizer_d = tf.train.AdamOptimizer(learning_rate, beta1=beta)\n",
    "#             self.opt_d = self.optimizer_d.minimize(self.loss_d, var_list=d_params)\n",
    "#             self.optimizer_g = tf.train.AdamOptimizer(learning_rate, beta1=beta)\n",
    "#             self.opt_g = self.optimizer_g.minimize(self.loss_g, var_list=g_params)\n",
    "\n",
    "            self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=None)\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "            # Launch the session\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "            self.sess.run(self.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T23:54:37.266584Z",
     "start_time": "2019-08-20T23:54:37.263541Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "discriminator = mlp_discriminator\n",
    "generator = point_cloud_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T21:33:11.034644Z",
     "start_time": "2019-08-20T21:33:10.816422Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "g_fc_channel = [128, 64, 128, 512, 1024, 6144]\n",
    "d_cov_channel = [3, 64, 128, 256, 256, 512]\n",
    "d_fc_channel = [512, 128, 64, 1]\n",
    "g_fc_weight = []\n",
    "for i in range(len(g_fc_channel) - 1):\n",
    "    in_c = g_fc_channel[i]\n",
    "    out_c = g_fc_channel[i + 1]\n",
    "    g_fc_weight.append(\n",
    "        (np.random.rand(in_c, out_c).astype(np.float32) * 0.1 - 0.05,\n",
    "         np.random.rand(out_c).astype(np.float32) * 0.1 - 0.05))\n",
    "\n",
    "d_cov_weight = []\n",
    "for i in range(len(d_cov_channel) - 1):\n",
    "    in_c = d_cov_channel[i]\n",
    "    out_c = d_cov_channel[i + 1]\n",
    "    d_cov_weight.append((np.random.rand(in_c, out_c).astype(np.float32) * 0.1 - 0.05,\n",
    "                         np.random.rand(out_c).astype(np.float32) * 0.1 - 0.05))\n",
    "\n",
    "d_fc_weight = []\n",
    "for i in range(len(d_fc_channel) - 1):\n",
    "    in_c = d_fc_channel[i]\n",
    "    out_c = d_fc_channel[i + 1]\n",
    "    d_fc_weight.append((np.random.rand(in_c, out_c).astype(np.float32) * 0.1 - 0.05,\n",
    "                        np.random.rand(out_c).astype(np.float32) * 0.1 - 0.05))\n",
    "\n",
    "input_noise = [np.random.rand(4, 128).astype(np.float32) * 0.1 - 0.05 for _ in range(10)]\n",
    "target_points = [\n",
    "    np.random.rand(4, 2048, 3).astype(np.float32) * 0.1 - 0.05 for _ in range(10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T21:33:13.287416Z",
     "start_time": "2019-08-20T21:33:11.042509Z"
    }
   },
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "tf.random.set_random_seed(0)\n",
    "model_opt = opt['model']\n",
    "if model_opt['type'] == 'wgan':\n",
    "    lam = 10\n",
    "    disc_kwargs = {'cov_init_list': d_cov_weight, 'fc_init_list': d_fc_weight}\n",
    "    gen_kwargs = {'init_list': g_fc_weight}\n",
    "    gan = PGAN(model_opt['type'],\n",
    "               train_opt['learning_rate'],\n",
    "               lam, [model_opt['num_points'], 3],\n",
    "               model_opt['noise_dim'],\n",
    "               discriminator,\n",
    "               generator,\n",
    "               disc_kwargs=disc_kwargs,\n",
    "               gen_kwargs=gen_kwargs,\n",
    "               beta=train_opt['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T21:32:55.101389Z",
     "start_time": "2019-08-20T21:32:53.571721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1327287e-07 -0.00828593\n",
      "-2.4586916e-07 -0.008225871\n",
      "-2.933666e-07 -0.008159315\n",
      "-3.6507845e-07 -0.008088458\n",
      "-4.3399632e-07 -0.008014243\n",
      "-4.833564e-07 -0.007936596\n",
      "-5.3923577e-07 -0.007855793\n",
      "-5.9977174e-07 -0.00777228\n",
      "-6.495975e-07 -0.007685996\n",
      "-7.21775e-07 -0.007597178\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    feed_dict = {gan.real_pc: target_points[i], gan.noise: input_noise[i]}\n",
    "    _, loss_d = gan.sess.run([gan.opt_d, gan.loss_d], feed_dict=feed_dict)\n",
    "    feed_dict = {gan.noise: input_noise[i]}\n",
    "    _, loss_g = gan.sess.run([gan.opt_g, gan.loss_g], feed_dict=feed_dict)\n",
    "    print(loss_d, loss_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T21:33:14.806377Z",
     "start_time": "2019-08-20T21:33:13.289588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1327287e-07 -0.00828593\n",
      "-2.4586916e-07 -0.008225888\n",
      "-2.924353e-07 -0.008159323\n",
      "-3.6600977e-07 -0.008088484\n",
      "-4.3120235e-07 -0.00801428\n",
      "-4.833564e-07 -0.007936636\n",
      "-5.3830445e-07 -0.0078557655\n",
      "-5.9977174e-07 -0.0077722715\n",
      "-6.421469e-07 -0.007686084\n",
      "-7.2224066e-07 -0.007597371\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    feed_dict = {gan.real_pc: target_points[i], gan.noise: input_noise[i]}\n",
    "    _, loss_d = gan.sess.run([gan.opt_d, gan.loss_d], feed_dict=feed_dict)\n",
    "    feed_dict = {gan.noise: input_noise[i]}\n",
    "    _, loss_g = gan.sess.run([gan.opt_g, gan.loss_g], feed_dict=feed_dict)\n",
    "    print(loss_d, loss_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T22:09:20.258698Z",
     "start_time": "2019-08-19T22:09:19.825154Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "feed_dict = {gan.real_pc: target_points[i], gan.noise: input_noise[i]}\n",
    "_, loss_d, loss_d_logit, gradient_penalty = gan.sess.run(\n",
    "    [gan.opt_d, gan.loss_d, gan.loss_d_logit, gan.gradient_penalty],\n",
    "    feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T22:17:29.473516Z",
     "start_time": "2019-08-19T22:17:29.468922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.999931335449219, 0.999993085861206, -2.1327286958694458e-07)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(loss_d), float(gradient_penalty), float(loss_d_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T22:18:24.497309Z",
     "start_time": "2019-08-19T22:18:24.478297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.041308507\n",
      "0.013890814\n",
      "0.0013969338\n",
      "0.041102726\n",
      "-0.040633775\n",
      "-0.045540597\n",
      "0.023586925\n",
      "0.03344834\n",
      "-0.009558369\n",
      "0.0005664937\n",
      "0.038841397\n",
      "0.020789754\n",
      "0.008475412\n",
      "0.032563414\n",
      "0.037454013\n",
      "0.0066774487\n"
     ]
    }
   ],
   "source": [
    "gen_var = gan.sess.run(tf.trainable_variables('wgan/dis'))\n",
    "for v in gen_var:\n",
    "    print(v.reshape(-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T21:10:42.455419Z",
     "start_time": "2019-08-19T21:10:42.419901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6817647 1.7762184 1.063653  1.252217 ]\n",
      " [2.2302346 2.4863346 1.6563923 1.8565607]]\n"
     ]
    }
   ],
   "source": [
    "# reset_tf_graph()\n",
    "# np.random.seed(0)\n",
    "# w = np.random.rand(3, 4).astype(np.float32)\n",
    "# b = np.random.rand(4).astype(np.float32)\n",
    "# in_f = np.random.rand(2, 3).astype(np.float32)\n",
    "\n",
    "# in_feat = tf.placeholder(tf.float32, [None, 3])\n",
    "# out = fully_connected(in_feat,\n",
    "#                       4,\n",
    "#                       weights_init=tf.constant_initializer(w),\n",
    "#                       bias_init=tf.constant_initializer(b))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     res = sess.run([out], feed_dict = {in_feat: in_f})\n",
    "# print(res[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_py3]",
   "language": "python",
   "name": "conda-env-tf_py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
